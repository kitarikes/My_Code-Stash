{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4kb3Wo6XZZvze7786QQpy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kitarikes/My_Code-Stash/blob/main/pytorch_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nnクラスの使い方"
      ],
      "metadata": {
        "id": "PJfkJnK1xj5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "kq-_YpycuM4k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMlp(nn.Module):\n",
        "  def __init__(self,\n",
        "               vec_length: int=16,\n",
        "               hidden_unit_1: int=8,\n",
        "               hidden_unit_2: int=2):\n",
        "    super(SimpleMlp, self).__init__()\n",
        "    \n",
        "    self.layer1 = nn.Linear(vec_length, hidden_unit_1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer2 = nn.Linear(hidden_unit_1, hidden_unit_2)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    out = self.layer1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer2(out)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "Ey8CX2LMukiQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = 16\n",
        "u1 = 8\n",
        "u2 = 2\n",
        "\n",
        "bs = 4\n",
        "\n",
        "x = torch.randn(bs, vec) #4*16"
      ],
      "metadata": {
        "id": "W1FjdP88weNT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1UBGXR8wojU",
        "outputId": "7e0127fa-9317-449b-abdd-68ba8e2bbea5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4357, -0.8163,  0.1530, -0.3898,  0.0692, -1.4470,  0.4773,  0.9313,\n",
              "          0.1823, -0.6926, -0.4840, -0.5682,  0.7434,  0.8738,  0.2804,  0.1215],\n",
              "        [ 0.1552, -1.0927, -0.3763, -0.5281, -0.0605,  0.5286, -2.3574,  1.1081,\n",
              "          0.0484, -0.7260,  1.0879,  0.3583, -1.9325,  1.0415,  0.1661, -0.6680],\n",
              "        [ 0.2197,  0.7555,  0.0322,  1.1035,  0.1629,  0.1661,  0.4502,  0.0176,\n",
              "         -0.4979,  0.7792, -0.5262,  0.3570, -0.0778,  0.0599, -0.4966, -0.5918],\n",
              "        [ 2.2016,  0.0503, -1.8721, -0.3182,  0.5474, -0.7765, -0.2650,  0.2996,\n",
              "          0.4807,  0.4485, -0.0668,  0.8647,  0.1631,  0.8188,  0.9984, -0.0612]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = SimpleMlp(vec, u1, u2)"
      ],
      "metadata": {
        "id": "IuSiQ20qw3T4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbF58u33xDGP",
        "outputId": "96f8a8dc-8cae-4067-bc5b-288f92c225bc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleMlp(\n",
              "  (layer1): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (layer2): Linear(in_features=8, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ0MPPRMxDpF",
        "outputId": "b6461772-19cd-42ef-c7a9-94f29d91dbeb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2400,  0.0343],\n",
              "        [ 0.3019,  0.1133],\n",
              "        [ 0.4721,  0.0036],\n",
              "        [ 0.3196, -0.1167]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4omXaeahxS3V"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Layer"
      ],
      "metadata": {
        "id": "tGtCp0_ZxqoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Uq0LWnLUxuQe"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VitInputLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels:int=3,\n",
        "               emb_dim:int=384,\n",
        "               num_patch_row:int=2,\n",
        "               image_size:int=32):\n",
        "    super(VitInputLayer, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.emb_dim = emb_dim\n",
        "    # 縦方向パッチ数\n",
        "    self.num_patch_row = num_patch_row\n",
        "    # 画像一片\n",
        "    self.image_size = image_size\n",
        "    # 総パッチの数\n",
        "    self.num_patch = self.num_patch_row**2\n",
        "    self.patch_size = int(self.image_size // self.num_patch_row)\n",
        "    print(f\"{self.patch_size}=\")\n",
        "\n",
        "    self.patch_emb_layer = nn.Conv2d(\n",
        "        in_channels=self.in_channels,\n",
        "        out_channels = self.emb_dim,\n",
        "        kernel_size=self.patch_size,\n",
        "        stride=self.patch_size\n",
        "    )\n",
        "\n",
        "      # クラストークン \n",
        "    self.cls_token = nn.Parameter(\n",
        "            torch.randn(1, 1, emb_dim) \n",
        "        )\n",
        "\n",
        "        # 位置埋め込み\n",
        "        ## クラストークンが先頭に結合されているため、\n",
        "        ## 長さemb_dimの位置埋め込みベクトルを(パッチ数+1)個用意 \n",
        "    self.pos_emb = nn.Parameter(\n",
        "            torch.randn(1, self.num_patch+1, emb_dim) \n",
        "        )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            x: 入力画像。形状は、(B, C, H, W)。[式(1)]\n",
        "                B: バッチサイズ、C:チャンネル数、H:高さ、W:幅\n",
        "        返り値:\n",
        "            z_0: ViTへの入力。形状は、(B, N, D)。\n",
        "                B:バッチサイズ、N:トークン数、D:埋め込みベクトルの長さ\n",
        "        \"\"\"\n",
        "        # パッチの埋め込み & flatten [式(3)]\n",
        "        ## パッチの埋め込み (B, C, H, W) -> (B, D, H/P, W/P) \n",
        "        ## ここで、Pはパッチ1辺の大きさ\n",
        "        z_0 = self.patch_emb_layer(x)\n",
        "        return z_0, self.patch_emb_layer.weight\n",
        "\n",
        "        ## パッチのflatten (B, D, H/P, W/P) -> (B, D, Np) \n",
        "        ## ここで、Npはパッチの数(=H*W/Pˆ2)\n",
        "        z_0 = z_0.flatten(2)\n",
        "\n",
        "        ## 軸の入れ替え (B, D, Np) -> (B, Np, D) \n",
        "        z_0 = z_0.transpose(1, 2)\n",
        "\n",
        "        # パッチの埋め込みの先頭にクラストークンを結合 [式(4)] \n",
        "        ## (B, Np, D) -> (B, N, D)\n",
        "        ## N = (Np + 1)であることに留意\n",
        "        ## また、cls_tokenの形状は(1,1,D)であるため、\n",
        "        ## repeatメソッドによって(B,1,D)に変換してからパッチの埋め込みとの結合を行う \n",
        "        z_0 = torch.cat(\n",
        "            [self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1)\n",
        "\n",
        "        # 位置埋め込みの加算 [式(5)] \n",
        "        ## (B, N, D) -> (B, N, D) \n",
        "        z_0 = z_0 + self.pos_emb\n",
        "        return z_0\n",
        "\n",
        "# batch_size, channel, height, width= 2, 3, 32, 32\n",
        "# x = torch.randn(batch_size, channel, height, width) \n",
        "# input_layer = VitInputLayer(num_patch_row=2) \n",
        "# z_0=input_layer(x)\n",
        "\n",
        "# # (2, 5, 384)(=(B, N, D))になっていることを確認。 \n",
        "# print(z_0.shape)\n"
      ],
      "metadata": {
        "id": "ON9kbnh2HqjI"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size, channel, height, width= 2, 3, 32, 32\n",
        "x = torch.randn(batch_size, channel, height, width) \n",
        "x.shape"
      ],
      "metadata": {
        "id": "RPISDDBDZBOE",
        "outputId": "43a27b55-8014-43e3-e2dd-575c748296f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = VitInputLayer(num_patch_row=2) \n",
        "\n",
        "y, w = input_layer(x)"
      ],
      "metadata": {
        "id": "J8qOfBlyYdw3",
        "outputId": "76f722af-3d08-4872-8e77-f19b40cb0df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape, w.shape"
      ],
      "metadata": {
        "id": "b_AWAwiRZ14X",
        "outputId": "bc52f7de-50e1-4195-c17a-734d105a0090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 384, 2, 2]), torch.Size([384, 3, 16, 16]))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.flatten(2).shape"
      ],
      "metadata": {
        "id": "oSFxYnqfZ4n5",
        "outputId": "11e975a1-21c8-442b-92ec-1c3006554d45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 384, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp = torch.randn((16*16, 16*16*3, 384))\n",
        "\n",
        "w_tmp.shape"
      ],
      "metadata": {
        "id": "UPXnMYmIdbha",
        "outputId": "4a4ec1ec-b959-401d-eb45-03102b9a66d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([384, 3, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(x, w_tmp)"
      ],
      "metadata": {
        "id": "q3z_yWle2jtZ",
        "outputId": "14a1e9c5-5165-4a29-f2b6-5ea5fd70aa7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-848e01329f96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (384) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rwZ3CUyU285P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}